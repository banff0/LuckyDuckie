<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/LuckyDuckie/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/LuckyDuckie/" rel="alternate" type="text/html" /><updated>2023-03-06T13:07:54-07:00</updated><id>http://localhost:4000/LuckyDuckie/feed.xml</id><title type="html">Ducks a driving</title><subtitle>This site hosts all my lab submissions for CMPUT 412</subtitle><entry><title type="html">Lab 3 Write Up</title><link href="http://localhost:4000/LuckyDuckie/2023/02/15/Lab-3.html" rel="alternate" type="text/html" title="Lab 3 Write Up" /><published>2023-02-15T00:00:00-07:00</published><updated>2023-02-15T00:00:00-07:00</updated><id>http://localhost:4000/LuckyDuckie/2023/02/15/Lab-3</id><content type="html" xml:base="http://localhost:4000/LuckyDuckie/2023/02/15/Lab-3.html">&lt;h1 id=&quot;exercise-3-report&quot;&gt;Exercise 3 Report&lt;/h1&gt;

&lt;p&gt;In this lab we accomplished a lot! We used OpenCV to be able to read and ID apriltags which we then used to help move the Duckiebot using lane following and landmark recognition. We also utilized RViz to view where the Duckiebot thought it was in the robot frame compared to the world frame as well visualize the previous apriltag detections.
&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;part-1&quot;&gt;Part 1&lt;/h2&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;computer-vision&quot;&gt;Computer Vision&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Below you can see a video of the Duckiebot detecting the apriltags. When this first started working, it was incredibly cool to see!&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/fT2It-2V_pQ&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;figcaption align=&quot;center&quot;&gt;&lt;b&gt;Vid.1: Video for apriltag detection&lt;/b&gt;&lt;/figcaption&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What does the apriltag library return to you for determining its position?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The apriltag library returns an AprilTag object for us! This is incredibly helpful since we are able to use commands like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apriltag.corners&lt;/code&gt; to help draw the bounding box of the image.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Which directions do the X, Y, Z values of your detection increase / decrease?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/LuckyDuckie/images_ex3/frame.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;figcaption align=&quot;center&quot;&gt;&lt;b&gt;Fig.1: Diagram to find tick rotation&lt;/b&gt;&lt;/figcaption&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As stated in the above picture, apriltag detections will increase on the &lt;em&gt;Z&lt;/em&gt; and &lt;em&gt;X&lt;/em&gt; axis as they move in the positive direction. apriltag detections will decrease as they move in the negative direction on the &lt;em&gt;Y&lt;/em&gt; axis.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What frame orientation does the apriltag use?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The camera frame is a translation and rotations away from the base of the robot frame so things change a little differently.&lt;/p&gt;

&lt;p&gt;Skimming over some of the nuance of the camera being pointed down a bit, as the robot moves towards positive X in the robot frame, the apriltag gets closer - or the position in Z in the camera frame decreases.&lt;/p&gt;

&lt;p&gt;If the robot rotates right, the apriltag detection will move towards negative x in the robot frame and vice-versa&lt;/p&gt;

&lt;p&gt;The frame orientation can be found using the right hand rule. Utilizing the RHR, we are able to see a frame orientation similar to Fig.1&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why are detections from far away prone to error?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Detections from far away can be prone to error due to a few reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The angle of the camera: Since the camera is tilted down, some distortion can occur when trying to detect tags&lt;/li&gt;
  &lt;li&gt;The field of view: With a wide field of view an image can look distorted when compared to a smaller FOV&lt;/li&gt;
  &lt;li&gt;The physical camera: The physical camera equipped on the Duckiebot is not the best quality. It also has a slight fisheye lens which can cause added distortion&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why may you want to limit the rate of detections?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Since our Duckiebot is not equipped with very much onboard memory, it can get easily overloaded by trying to detect many objects in a small period of time. By limiting the number of detections, we can increase our operating efficeny and also speed up subsequent detections.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;part-2&quot;&gt;Part 2&lt;/h2&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;lane-following&quot;&gt;Lane Following&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Below you can see a video of the Duckiebot utilizing the PID controller and lane following &lt;u&gt;American&lt;/u&gt; driver style.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/p1k3Xm_qgXU&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;figcaption align=&quot;center&quot;&gt;&lt;b&gt;Vid.2: Video for American style driver&lt;/b&gt;&lt;/figcaption&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Below you can see a video of the Duckiebot utilizing the PID controller and lane following &lt;u&gt;English&lt;/u&gt; driver style.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/xrq4UWQTJRE&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;figcaption align=&quot;center&quot;&gt;&lt;b&gt;Vid.3: Video for English style driver&lt;/b&gt;&lt;/figcaption&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;For both of the above videos, there are a few subtlties to take note of:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A vertical blue line down the centre of the screen to visualize the centre of the camera on the Duckiebot&lt;/li&gt;
  &lt;li&gt;Green rectangles to track the closest lane segment. If two rectangles are present, the closest one is ignored&lt;/li&gt;
  &lt;li&gt;A green line connecting the green square to the vertical blue line to visualize approximately how far off we are from centre&lt;/li&gt;
  &lt;li&gt;A red dot rapidly moving around the screen to visualize how much rotation the Duckiebot must make to stay on the correct side of the road&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is the error for your PID controller?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This error is found by the following general equation:
&lt;script type=&quot;math/tex&quot;&gt;\begin{equation}
error = (x + (sizeratio*y)) - goal) * pixelscale
\end{equation}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$x$ = the left most side of the closest dotted line&lt;/li&gt;
  &lt;li&gt;$y$ = the top of closest dotted line divided by two&lt;/li&gt;
  &lt;li&gt;$sizeratio$ = handles distance distortion. It creats a linear function describing how close the dotted line is vs how close the Duckiebot should be to it. This address the vanishing point problem.&lt;/li&gt;
  &lt;li&gt;$goal$ = where the bot wants to be. Represented by a blue vertical line in the image&lt;/li&gt;
  &lt;li&gt;$pixelscale$ = scales the error down to account for large error in pixels realative to a small error in angle&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If your proportional controller did not work well alone, what could have caused this?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For our purposes a proportional controller worked well. If our system contained momentum, this could have caused overshoot. However, adding a derivative term and accounting for derivative kick could solve the momentum problem.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Does the D term help your controller logic? Why or why not?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Currently it does not help our controller since we did not have time to use it. Since our system does not consider a force like momentum, the P term is more than sufficent.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why or why not was the I term useful for your robot?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While we did not implement the I term into this project it could have potentially been useful. Since the I term tries to compensate for error over time this could be useful when there is some error in the wheel encoders or slippage on the mat. If we did consider using an I term, we would also have to be wary of integral windup. To combat this, we would limit how large the I term could be.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;part-3&quot;&gt;Part 3&lt;/h2&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;localization-using-sensor-fusion&quot;&gt;Localization using Sensor Fusion&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Below you can see view from RViz as we do a lap around the track. The camera feed, odometry frame and static apriltag frame is shown. In this video, the moving object is our Duckiebot being controlled manually. The static objects shown are the apriltags in our world frame.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/l8UcIMxtJF0&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;figcaption align=&quot;center&quot;&gt;&lt;b&gt;Vid.4: Localization using static apriltags&lt;/b&gt;&lt;/figcaption&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Where did your odometry seem to drift the most? Why would that be?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The most drift was seen when turning. The necessity for encoder precision is maximized in the turns since small errors can compound and cause large drift.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Did adding the landmarks make it easier to understand where and when the odometry
drifted?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Adding the landmarks made it significantly easier! Being able to visualize where the robot is in real-time is very beneficial.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Show the generated transform tree graph, what is the root/parent frame?&lt;/strong&gt;&lt;/p&gt;
&lt;object data=&quot;/LuckyDuckie/images_ex3/originalTransformTree.pdf&quot; width=&quot;150%&quot; height=&quot;500px&quot; type=&quot;application/pdf&quot;&gt;&lt;/object&gt;
&lt;figcaption align=&quot;center&quot;&gt;&lt;b&gt;Fig.2: Original transform tree&lt;/b&gt;&lt;/figcaption&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;There are currently two root frames - the first being the “{robot_name}/WorldFrame” frame that we created for the odometry and apriltag locations. The second was the “{robot_name} footprint” frame that contained all of the components of the bot are children of. It made sense to set the parent of the footprint frame to be the odometry frame, since we then would be able to visualize all components of the robot relative to the apriltag locations.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Move the wheels and make note of which join is moving. What type of joint is this?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To determine this, first we set footprint’s parent frame to be the odometry frame. For some time we tried to figure out how to connect the footprint of the bot to the odometry frame so the bot is placed where it perceived itself to be in the world. Using static transforms (similar to the apriltags) made this job possible. Plus, we use this same method further on! Being able to utilize all of this helped us to find that almost every joint type is fixed but the wheel joints are continuous.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;You may notice that the wheel frames rotate when you rotate the wheels, but the frames never move from the origin? Even if you launch your odometry node the Duckiebot’s frames do not move. Why is that?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This was due to the two root nodes. We could set RViz to show the footprint of the Duckiebot OR show the WorldFrame with the odometry node. We need to connect the footprint to the odometry node for this to link properly.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What should the translation and rotation be from the odometry child to robot parent frame? In what situation would you have to use something different?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this case, we want to use a translation of $(0,0,0)$ and a rotation of $(0,0,0)$. The odometry node shows precisely where the bot thinks it is, so no rotation or translation is needed. For example, we would need a translation or rotation if the odometry node was publishing the robot frame 90° off of expected.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;After creating this link generate a new transform tree graph. What is the new root/parent frame for your environment?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The new root is just footprint. The key difference here is that the WorldFrame, apriltags, and odometry node are not appearing as a separate tree. We understand it to be that the “tf2_tools view_frames.py” script is hardcoded to assume the root is footprint, so even though footprint now has a parent odometry_node, it is not shown.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Can a frame have two parents? What is your reasoning for this?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;No. The child frame needs to be positioned relative to the parent. If there are two parents that have two different positions, what does it mean to be $(+0.5, +0.5)$ transformed from both parents? This would give an inconclusive result or a bad coordinate. However, a parent can have multiple children; there’s no issue having multiple children reference a single parent’s location.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Can an environment have more than one parent/root frame?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It can, but it is highly recommended not to. Issue visualizing in RViz will occur as well as issues in testing and debugging the code.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Show the newly generated transform tree graph, what is the new root/parent frame?&lt;/strong&gt;&lt;/p&gt;

&lt;object data=&quot;/LuckyDuckie/images_ex3/newTransformTree.pdf&quot; width=&quot;150%&quot; height=&quot;500px&quot; type=&quot;application/pdf&quot;&gt;&lt;/object&gt;
&lt;figcaption align=&quot;center&quot;&gt;&lt;b&gt;Fig.3: New transform tree&lt;/b&gt;&lt;/figcaption&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The new parent frame shown on the transform tree graph is footprint - but in reality the root is “WorldFrame”. You can imagine just drawing a connection between “odometry” and “footprint” in &lt;strong&gt;Enter figure number here!&lt;/strong&gt; - this is the true new tree.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Below you can see a video of the Duckiebot moving around the world frame with all robot frames attached to the moving odometry frame. Apriltag detections are also shown in the camera feed and visualized in RViz.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/gwi9RyucWMo&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;figcaption align=&quot;center&quot;&gt;&lt;b&gt;Vid.5: Localization using dynamic apriltags &lt;/b&gt;&lt;/figcaption&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How far off are your detections from the static ground truth?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As seen in Vid.5, our detections are not incredibly accurate. Being able to see the physical location of the apriltags on the camera as well as the perceived location on RViz lets us know we have a bit of work to do on detections&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What are two factors that could cause this error?&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The code to convert the 2d bounding box + depth to the 3d coordinate from the camera doesn’t account for distortion of the lens.&lt;/li&gt;
  &lt;li&gt;Inaccuracies converting from the camera frame such as not accounting for camera being pointed just slightly down.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In the future we will add a video here of the Duckiebot moving around the world using lane following. Our sensor fusion node attempts to teleport the robot if an apriltag is found and to use odometry if no apriltag is detected. Our goal is to finish as close to the start as possible.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Is this a perfect system?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While this isn’t a perfect system, it is not bad. By using apriltag teleportation with a combination of dead reckoning and computer vision, we can get a fairly good lane following system. There are some definite improvements and tweaks we hope to make in the future.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What are the causes for some of the errors?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Some causes for the error include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Memory usage on the Duckiebot: In some scenarios, the Duckiebot seems to get a little bit overloaded and not have enough processing power to compute all the commands given to it at one time. When running manual control + RViz, we noticed some significant delay.&lt;/li&gt;
  &lt;li&gt;Human tuning: Since we are tuning some constants by hand/inspection, the values are not optimal. Possibly by using machine learning we could fix this problem.&lt;/li&gt;
  &lt;li&gt;Unaccounted distortion on camera: This can cause inaccuracies with our apriltag detections and our lane following.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What other approaches could you use to improve localization?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To improve localization we could:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Use multiple sensors or improved sensors. By using signals like GPS we can have a more accurate reading of where we are&lt;/li&gt;
  &lt;li&gt;Use machine learning. By training our bot we can have it better estimate it’s position&lt;/li&gt;
  &lt;li&gt;Combination approach. By using a combination of improved sensors, machine learning, better data etc we can improve our localization by a large magnitude.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;a href=&quot;https://eclass.srv.ualberta.ca/pluginfile.php/9276727/mod_resource/content/3/Exercise%203.pdf&quot;&gt;Lab Manual&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docsduckietown.org/daffy/duckietown-classical-roboticsduckietown-classical-robotics-ready-tablet.pdf&quot;&gt;Classical Robotics Architectures using Duckietownown&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pyimagesearch.com/2020/11/02/apriltag-with-python/&quot;&gt;Apriltag with Python&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/duckietown/dt-core&quot;&gt;dt-core Library&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.google.com/document/d/1bQfkR_tmwctFozEZlZkmojBZHkegscJPJVuw-IEXwI4/edit#&quot;&gt;CMPUT 412 Cheat Sheet&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.duckietown.org/daffy/&quot;&gt;Duckietown Docs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.opencv.org/4.x/&quot;&gt;OpenCV Docs&lt;/a&gt;&lt;/p&gt;

&lt;!-- &lt;h1&gt; References: &lt;/h1&gt;
&lt;p&gt;
&lt;ul&gt;
    
        &lt;li&gt;
            &lt;a href=&quot;https://docs.duckietown.org/daffy/duckietown-classical-robotics/out/cra_basic_augmented_reality_exercise.html&quot; target=&quot;_blank&quot;&gt;https://docs.duckietown.org/daffy/duckietown-classical-robotics/out/cra_basic_augmented_reality_exercise.html&lt;/a&gt; 
        &lt;/li&gt;
    
        &lt;li&gt;
            &lt;a href=&quot;https://pyimagesearch.com/2020/11/02/apriltag-with-python/&quot; target=&quot;_blank&quot;&gt;https://pyimagesearch.com/2020/11/02/apriltag-with-python/&lt;/a&gt; 
        &lt;/li&gt;
    
        &lt;li&gt;
            &lt;a href=&quot;https://github.com/duckietown/dt-core/blob/6d8e99a5849737f86cab72b04fd2b449528226be/packages/ground_projection/src/ground_projection_node.py#L46&quot; target=&quot;_blank&quot;&gt;https://github.com/duckietown/dt-core/blob/6d8e99a5849737f86cab72b04fd2b449528226be/packages/ground_projection/src/ground_projection_node.py#L46&lt;/a&gt; 
        &lt;/li&gt;
    
        &lt;li&gt;
            &lt;a href=&quot;https://github.com/duckietown/lib-dt-apriltags/blob/master/dt_apriltags/apriltags.py&quot; target=&quot;_blank&quot;&gt;https://github.com/duckietown/lib-dt-apriltags/blob/master/dt_apriltags/apriltags.py&lt;/a&gt; 
        &lt;/li&gt;
    
        &lt;li&gt;
            &lt;a href=&quot;https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html&quot; target=&quot;_blank&quot;&gt;https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html&lt;/a&gt; 
        &lt;/li&gt;
    
&lt;/ul&gt;
&lt;/p&gt; --&gt;</content><author><name></name></author><summary type="html">Exercise 3 Report</summary></entry><entry><title type="html">Lab 2 Write Up</title><link href="http://localhost:4000/LuckyDuckie/2023/02/12/Lab-2-writeup.html" rel="alternate" type="text/html" title="Lab 2 Write Up" /><published>2023-02-12T00:00:00-07:00</published><updated>2023-02-12T00:00:00-07:00</updated><id>http://localhost:4000/LuckyDuckie/2023/02/12/Lab-2-writeup</id><content type="html" xml:base="http://localhost:4000/LuckyDuckie/2023/02/12/Lab-2-writeup.html">&lt;!-- &lt;h1&gt; Lab 2 Write Up &lt;/h1&gt; --&gt;

&lt;p&gt;
        In this lab we implemented the odometry of the duckiebot, as well as use the kinematics of the bot to get it to move in a specified pattern. The first step in doing this was to track how far we had gone in meters. To do this we got the number of ticks recorded by the wheels by subscribe to the topics /left_wheel_encoder_node/tick and /right_wheel_encoder_node/tick then using this data we were able to calculate the distance traveled by out bot by using the formula (2*pi*wheel_raduis*ticks)/135 for each of the wheels . Using those distances we were also able to track the angle of our robot by using the formula ((distance_right - distance_left) / (2 * axel_radius)). Putting these two things together we ended up with the vector for the robot frame (where for the duckiebot yr is always 0) setting the xr to be the distance traveled. In doing this an issue arose where the distance continually increased since the ticks had no way of distinguishing between forward and backward motion, to remedy this we tracked the direction of the wheel based on the velocities we set, and used this to compute the direction of movement of each wheel. In order to get the wheel to rotate at a set velocity we then needed to publish a “WheelsCmdStamped” message to the topic wheels_driver_node/wheels_cmd which has a subscriber that will set the wheel velocities on the physical robot. At this point I should note that Jonathan and I became partners towards the end of the lab, so we both completed the above functionality in slightly different manners, but ultimately decided to complete the lab using his codebase. The last thing that needed to be figured out was how to change the colour of the LED on the bot, so that we could indicate which state it was in. To do this Jonathan created a service proxy that was able to communicate with the led_emitter_node/set_pattern topic and give us control of the bot’s lights. 

&lt;/p&gt;
&lt;p&gt;
        Once we knew where the bot was and how to move it we could use this to execute commands such as move forward 1 meter or rotate 180 degrees. To do this I implemented a function that would check if the robot had met the parameters required for the previous command, and if so we would then set the wheel velocity to the necessary speeds for the next command, mark the current distance and angle of the bot -- so that we could track how far the bot had to go until it reached the distance or angle required, and update the onboard LED to indicate the new command being executed. I then placed all the commands needed to complete the predetermined pattern in a list, with each command consisting of a dictionary that held all the information needed (when to finish the previous command and begin executing, the wheel speed required to complete the task, and the LED colour assigned to the command. Then the bot just traversed this list and executed the commands until the list was empty. 
 


&lt;/p&gt;

&lt;p&gt;
    At this point a number of issues were encountered, firstly the bot was not moving in a straight line when the wheel speeds were set to the same value. To rectify this issue we set one wheel to have less power than the other, this straightened out the bot after some trial and error. The second issue was the bot kept on overshooting the goal destinations we set in the commands, either going past the expected distance, or over rotating. To rectify this we tried increasing the threshold on when we could say the bot was sufficiently close to an angle or position and begin executing the next command. This worked to some extent, but often resulted in an over correction where the bot would under rotate, or not move far enough. The best solution we found was to set the wheel velocities to be lower, then the slower moving bot was less likely to move past a goal before the bot could get the ticks, calculate the distance, and then publish the next velocity to the wheels. There was still an issue of the movement being rather unpredictable, and we suspected this might be because some momentum from the previous command was being carried over, but unaccounted for, resulting in the bot moving in an unpredicted manner. To test this we put a pause between each of the bots commands, and this resulted in much more predictable movement from the bot confirming our initial suspicions Since this yielded better results this pausing was left in the final codebase. Lastly we then took the robot frame that we had been working in and converted that to the world frame, using the inverse kinematics provided in class. 
&lt;/p&gt;

&lt;p&gt;
    We tried to get the rosbag working but Jonathan and I ran out of time, and Huayie was unable to figure out how to do it as well. 

&lt;/p&gt;

&lt;h2&gt; Answers to Questions: &lt;/h2&gt;

&lt;ul&gt;
    &lt;li&gt;
        To translate between robot frame and initial frame you set 
        world_x = robot_x * cos(robot theta)
        world_y = robot_y * cos(robot theta)
        Wold_theta = robot_theta
        Then for this example, since we started at x = 0.32, y = 0.32 we need to add 0.32 to both world_x and world_y
    &lt;/li&gt;
    &lt;li&gt;
        To convert the location and theta of the initial position to the world frame you take robot_x which == 0, thus world_x and world_y are zero plus the 0.32 we add to account for the starting position, and then set the world theta to be the robot theta.
    &lt;/li&gt;
    &lt;li&gt;
        There is a difference between the actual location of out bot, and the desired location since the robot's movement is not precise, so the wheels might slip, or there may be a loss in traction, this leads to the robot thinking it has moved when in reality it has not causing this difference. 
    &lt;/li&gt;
    &lt;li&gt;
        The topic used to make the motor move is vehical_name/wheels_driver_node/wheels_cmd and the way we figured this out was by looking at the different topics, and then checking the information on ones with names dealing with wheels, and if the type had a structure that seemed to be in the correct format to make it move and there was a subscriber, we tried sending it a message to move the wheels.
    &lt;/li&gt;
    &lt;li&gt;
        We used a speed of 0.5 for both wheels, an increase in speed led to a decrease in accuracy, and too low a speed led to the bot not being able to move under its own weight. 
    &lt;/li&gt;
    &lt;li&gt;
        To track the angle rotated we used the formula: 
        ((distance_right - distance_left) / (2 * axel_radius))
        Where ditance_left/right was calculated as:
            (2 * pi * wheel_radius * ticks) / 135
    &lt;/li&gt;
    &lt;li&gt;
        To make the robot rotate we set the speed of the two wheels in opposite directions, resulting in neutral steering, thus to do this we used the same topic used to make the bot move forward (vehical_name/wheels_driver_node/wheels_cmd)
    &lt;/li&gt;
    &lt;li&gt;
        To track the angle rotated we set a variable to 0 initially and then updated it to the rotation angle calculated above every time a new message was published to the wheel tick topic.
    &lt;/li&gt;
    &lt;li&gt;
        To drive the robot in a circle we just need to set the velocity of the right, and left wheel to be different speeds in the same direction, then the ratio of these two speeds will lead to a different radius of the circle. 
    &lt;/li&gt;
    &lt;li&gt;
        The final position of the robot was:
        World_x = 7.414957177079178
        World_y = 5.215506373748548
        World_theta = 2.5376110196153103
        This was not that close to the actual position of the robot.
    &lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt; Images/Videos: &lt;/h2&gt;

&lt;h1&gt; Screenshot of camera image view in my own topic &lt;/h1&gt;
&lt;ul&gt;
    &lt;p&gt;  This is an &lt;a href=&quot;https://drive.google.com/file/d/1XDtMp3fwPTiP-rHIKWO8881Wl9WSTcOd/view?usp=share_link&quot; target=&quot;_blank&quot;&gt;image&lt;/a&gt; of the image that I published to my own topic seen through rqt_image, its exactly the same as the image from the compressed image topic, since that is what I subscribed to in order to get my image.
    &lt;/p&gt;
&lt;/ul&gt;

&lt;h1&gt; Screenshot of code that shows how to acquire and send images using ros topics &lt;/h1&gt;
&lt;ul&gt;
    &lt;p&gt; This &lt;a href=&quot;https://drive.google.com/file/d/1yg21LQmskpOFAGkV4Rl7wxPDX1oDg6Y6/view?usp=share_link&quot; target=&quot;_blank&quot;&gt;image&lt;/a&gt; shows the code that allows us to communicate with a ros topic by subscribing to it, in this case an image topic, and then it publishes that image to a differnet topic so that it can be accessed by a different subscriber.
    &lt;/p&gt;
&lt;/ul&gt;

&lt;h1&gt; Videos of my Duckiebot performing the task defined in part two &lt;/h1&gt;
&lt;ul&gt;
    &lt;!-- &lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;https://youtu.be/BzEZb2VpFs4&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt; --&gt;
    &lt;p&gt; This &lt;a href=&quot;https://drive.google.com/file/d/1wiUuJ0EwKTymeTDd-oVdGuFo8J_a2tH7/view?usp=share_link&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt; shows how we got the duckiebot to follow the set path given in the assignment, there is a lot of deviation from the expected path due to the imprefections of the duckiebot (loss of traction, gear slippage, ect) but for the most part it does what we want and is able to traverse a space given commends. Also here is another video where it performs some parts better than it did in the first video, with a little help from Jonathan &lt;a href=&quot;https://drive.google.com/file/d/1wrnlKZkeCrh3XfT4JkjrmiMjoSXX707V/view?usp=share_link&quot; target=&quot;_blank&quot;&gt;video 2&lt;/a&gt;, and one more where there is an example of the bot not being able to rotate under its own weight &lt;a href=&quot;https://drive.google.com/file/d/1whm4RT-zLn-frwvhL6uOuf0UWlyXqN7P/view?usp=share_link&quot; target=&quot;_blank&quot;&gt;video 3&lt;/a&gt; &lt;/p&gt;
&lt;/ul&gt;

&lt;h1&gt; Videos of some of the smaller exercises in part one &lt;/h1&gt;
&lt;ul style=&quot;list-style-type: none&quot;&gt;
    &lt;li&gt;
        &lt;p&gt; This is the &lt;a href=&quot;https://drive.google.com/file/d/1wueelVZc_WT6KOLfMjTpzYkspeM-Zty1/view?usp=share_link&quot; target=&quot;_blank&quot;&gt;move one meter then back video&lt;/a&gt; where the duckiebot moved one meter forward, then one back, following pre defined commands, and no human intervention. &lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
        &lt;p&gt; This is the &lt;a href=&quot;https://drive.google.com/file/d/1x2SlyQsovt5KSnkOuLvoV_w1oDzd071M/view?usp=share_link&quot; target=&quot;_blank&quot;&gt;rotate 90 degrees video&lt;/a&gt; where the duckiebot rotated 90 degrees following pre defined commands, and no human intervention. &lt;/p&gt;
    &lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt; References: &lt;/h1&gt;
&lt;p&gt;
&lt;ul&gt;
    
        &lt;li&gt;
            &lt;a href=&quot;https://github.com/duckietown/dt-core&quot; target=&quot;_blank&quot;&gt;https://github.com/duckietown/dt-core&lt;/a&gt; 
        &lt;/li&gt;
    
        &lt;li&gt;
            &lt;a href=&quot;https://github.com/duckietown/dt-core/blob/daffy/packages/led_emitter/src/led_emitter_node.py&quot; target=&quot;_blank&quot;&gt;https://github.com/duckietown/dt-core/blob/daffy/packages/led_emitter/src/led_emitter_node.py&lt;/a&gt; 
        &lt;/li&gt;
    
        &lt;li&gt;
            &lt;a href=&quot;https://github.com/duckietown/template-ros&quot; target=&quot;_blank&quot;&gt;https://github.com/duckietown/template-ros&lt;/a&gt; 
        &lt;/li&gt;
    
        &lt;li&gt;
            &lt;a href=&quot;http://wiki.ros.org/ROS/Concepts&quot; target=&quot;_blank&quot;&gt;http://wiki.ros.org/ROS/Concepts&lt;/a&gt; 
        &lt;/li&gt;
    
        &lt;li&gt;
            &lt;a href=&quot;https://docs.duckietown.org/daffy/duckietown-robotics-development/out/dt_infrastructure.html&quot; target=&quot;_blank&quot;&gt;https://docs.duckietown.org/daffy/duckietown-robotics-development/out/dt_infrastructure.html&lt;/a&gt; 
        &lt;/li&gt;
    
        &lt;li&gt;
            &lt;a href=&quot;https://docs.duckietown.org/daffy/duckietown-robotics-development/out/odometry_modeling.html&quot; target=&quot;_blank&quot;&gt;https://docs.duckietown.org/daffy/duckietown-robotics-development/out/odometry_modeling.html&lt;/a&gt; 
        &lt;/li&gt;
    
        &lt;li&gt;
            &lt;a href=&quot;http://wiki.ros.org/rospy_tutorials/Tutorials/WritingImagePublisherSubscriber&quot; target=&quot;_blank&quot;&gt;http://wiki.ros.org/rospy_tutorials/Tutorials/WritingImagePublisherSubscriber&lt;/a&gt; 
        &lt;/li&gt;
    
&lt;/ul&gt;
&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Lab 1 Write Up</title><link href="http://localhost:4000/LuckyDuckie/2023/01/22/first-post.html" rel="alternate" type="text/html" title="Lab 1 Write Up" /><published>2023-01-22T00:00:00-07:00</published><updated>2023-01-22T00:00:00-07:00</updated><id>http://localhost:4000/LuckyDuckie/2023/01/22/first-post</id><content type="html" xml:base="http://localhost:4000/LuckyDuckie/2023/01/22/first-post.html">&lt;p&gt;
        In this lab we implemented the tools and setup to allow us to access the duckiebots using our computer, this included setting up the correct environments on our laptops (docker, github, ROS connection, duckitown terminal, ect.). We also learnt some of the key dts commands that allow us to interface with the duckiebots, drive command, camera command, ect. Another key step in this lab was to set up the duckiebots, though they came pre assembled we still needed to make sure to calibrate the bots, this included making sure the camera inputs were calibrated, the
        wheels were calibrated and the line recognition was set up correctly. We also set up some preferences on the duckiebot to make sure the gain and trim were correct for our purposes. Finally we learnt how to run code on the duckiebot that we had written on our laptops.
&lt;/p&gt;
&lt;p&gt;
        Some of the challenges faced in this lab were after I had done all the set up and began the lane follow command the duckiebot worked fine the first time, but following that the next few subsequent tests resulted in the bot not following the lines at all. I resolved this issue
        by relcallibrating the wheel trim and this seemed to resolve the issue leading me to believe this was not an issue with the line recognition.
        The biggest challenge by far though was trying to get the dockerhub image working. My first issue was that I had not read that I needed to change the version we were using to arm64v8 which resulted in the download for opencv taking a very long time. Once I had made the appropriate adjustments I was able to install opencv with no issues. The next issue I encountered was trying to get the boilerplate code for accessing the duckiebot camera with opencv to work. This proved difficult even after consenting the github repo where they set up the gst pipeline() which seemed like it should have worked but did not. 


&lt;/p&gt;

&lt;p&gt;
    In this lab we learnt how to access the duckiebots from our laptops and get them to run code we had written on our laptops, meaning that in the future we should be able to make them do rather cool things. We also learnt the basics of docker and how it allows us to work in a separate environment that is easily shareable to a separate device(such as a duckiebot).
&lt;/p&gt;

&lt;h2&gt; Dashboard image &lt;/h2&gt;
&lt;p&gt; Sorry! I could'nt get the image embeded here, I hope to be more profficient with jekyll by next lab so my site should look a little nicer here is a link to the &lt;a href=&quot;https://drive.google.com/file/d/1B91bwVNb5j3sig1g9VRmeKbYeoHKD7Jr/view?usp=share_link&quot; target=&quot;_blank&quot;&gt;image&lt;/a&gt;: 
&lt;/p&gt;

&lt;h2&gt; Drive for 2 meters &lt;/h2&gt;
&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;https://youtu.be/BzEZb2VpFs4&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;
&lt;p&gt; &lt;a href=&quot;https://youtu.be/BzEZb2VpFs4&quot; target=&quot;_blank&quot;&gt;Link&lt;/a&gt; in case the above video does not work &lt;/p&gt;

&lt;h2&gt; Lane follow demo &lt;/h2&gt;
&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;https://youtu.be/5u_FgfXBX1M&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt; &lt;a href=&quot;https://youtu.be/5u_FgfXBX1M&quot; target=&quot;_blank&quot;&gt;Link&lt;/a&gt; in case the above video does not work &lt;/p&gt;</content><author><name></name></author><summary type="html">In this lab we implemented the tools and setup to allow us to access the duckiebots using our computer, this included setting up the correct environments on our laptops (docker, github, ROS connection, duckitown terminal, ect.). We also learnt some of the key dts commands that allow us to interface with the duckiebots, drive command, camera command, ect. Another key step in this lab was to set up the duckiebots, though they came pre assembled we still needed to make sure to calibrate the bots, this included making sure the camera inputs were calibrated, the wheels were calibrated and the line recognition was set up correctly. We also set up some preferences on the duckiebot to make sure the gain and trim were correct for our purposes. Finally we learnt how to run code on the duckiebot that we had written on our laptops. Some of the challenges faced in this lab were after I had done all the set up and began the lane follow command the duckiebot worked fine the first time, but following that the next few subsequent tests resulted in the bot not following the lines at all. I resolved this issue by relcallibrating the wheel trim and this seemed to resolve the issue leading me to believe this was not an issue with the line recognition. The biggest challenge by far though was trying to get the dockerhub image working. My first issue was that I had not read that I needed to change the version we were using to arm64v8 which resulted in the download for opencv taking a very long time. Once I had made the appropriate adjustments I was able to install opencv with no issues. The next issue I encountered was trying to get the boilerplate code for accessing the duckiebot camera with opencv to work. This proved difficult even after consenting the github repo where they set up the gst pipeline() which seemed like it should have worked but did not.</summary></entry><entry><title type="html">Welcome to Jekyll!</title><link href="http://localhost:4000/LuckyDuckie/jekyll/update/2023/01/16/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2023-01-16T14:09:29-07:00</published><updated>2023-01-16T14:09:29-07:00</updated><id>http://localhost:4000/LuckyDuckie/jekyll/update/2023/01/16/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/LuckyDuckie/jekyll/update/2023/01/16/welcome-to-jekyll.html">&lt;p&gt;You’ll find this post in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.</summary></entry></feed>